# Monitoring Your Mistral API Token Usage for the Hackathon

During this hackathon, one of the evaluation criteria involves tracking and reporting your team's Mistral API token consumption. This guide explains how to access token usage information from the Mistral API responses and log it into a standard CSV file.

**Why Monitor Usage?**

1.  **Evaluation:** It's part of the hackathon's judging criteria.
2.  **Cost Awareness:** Understanding token usage helps manage potential costs associated with API keys.
3.  **Efficiency:** It encourages building more token-efficient prompts and applications.

## How Mistral Reports Usage

When you make a successful call to Mistral's primary endpoints like Chat Completions (`/v1/chat/completions`), the API response includes a `usage` object. This object details the number of tokens processed for that specific request.

**Example Response Snippet (`/v1/chat/completions`)**

```json
{
  "id": "cmpl-xxxxxxxx",
  "object": "chat.completion",
  "created": 170xxxxxxx,
  "model": "mistral-small-latest", // <-- The model used
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The capital of France is Paris."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": { // <-- THIS IS THE KEY PART!
    "prompt_tokens": 15, // <-- Tokens in your input prompt
    "completion_tokens": 8, // <-- Tokens in the generated response
    "total_tokens": 23 // <-- Sum of prompt and completion tokens
  }
}
```

**Key Fields:**

*   `usage.prompt_tokens`: Number of tokens in the input messages you sent.
*   `usage.completion_tokens`: Number of tokens in the response generated by the model.
*   `usage.total_tokens`: The sum of `prompt_tokens` and `completion_tokens`.
*   `model`: The specific Mistral model used for the request (needed for potential cost calculation).

## Accessing Usage Data in Python

You need to capture the response object from your API call and then access the `usage` fields.

**Method 1: Using the Official `mistralai` Client Library**

If you're using the `mistralai` library, the response object directly exposes the usage details.

```python
import os
import csv
import datetime
from mistralai.client import MistralClient
from mistralai.models.chat_completion import ChatMessage

# --- Configuration ---
API_KEY = os.environ.get("MISTRAL_API_KEY", "YOUR_MISTRAL_API_KEY") # Replace or use env var
MODEL_NAME = "mistral-small-latest"
LOG_FILE = "mistral_usage_log.csv"
# --- End Configuration ---

client = MistralClient(api_key=API_KEY)

def call_mistral_and_log(user_prompt):
    """Makes an API call and logs usage."""
    messages = [ChatMessage(role="user", content=user_prompt)]

    try:
        # Make the API call
        chat_response = client.chat(
            model=MODEL_NAME,
            messages=messages,
        )

        # --- Usage Monitoring ---
        model_used = chat_response.model
        prompt_tokens = chat_response.usage.prompt_tokens
        completion_tokens = chat_response.usage.completion_tokens
        total_tokens = chat_response.usage.total_tokens

        print(f"Model Used: {model_used}")
        print(f"Prompt Tokens: {prompt_tokens}")
        print(f"Completion Tokens: {completion_tokens}")
        print(f"Total Tokens: {total_tokens}")

        # Log this usage to CSV
        log_usage_to_csv(model_used, prompt_tokens, completion_tokens, total_tokens)
        # --- End Usage Monitoring ---

        # Return the actual content
        return chat_response.choices[0].message.content

    except Exception as e:
        print(f"Error calling Mistral API: {e}")
        # Optionally log errors separately, but don't log token usage for failed calls
        return None

# --- CSV Logging Function ---
import threading
log_lock = threading.Lock() # Use lock if making concurrent calls

def log_usage_to_csv(model, prompt_tokens, completion_tokens, total_tokens):
    """Appends usage data to the CSV log file."""
    timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat()
    fieldnames = ["TimestampUTC", "Model", "PromptTokens", "CompletionTokens", "TotalTokens"]
    data_row = {
        "TimestampUTC": timestamp,
        "Model": model,
        "PromptTokens": prompt_tokens,
        "CompletionTokens": completion_tokens,
        "TotalTokens": total_tokens,
    }

    with log_lock: # Ensure thread-safe file writing
        file_exists = os.path.isfile(LOG_FILE)
        with open(LOG_FILE, 'a', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            if not file_exists or os.path.getsize(LOG_FILE) == 0:
                writer.writeheader() # Write header only if file is new/empty
            writer.writerow(data_row)
    print(f"Logged usage to {LOG_FILE}")

# --- Example Usage ---
if __name__ == "__main__":
    prompt = "What is the capital of France?"
    response_content = call_mistral_and_log(prompt)
    if response_content:
        print(f"\nMistral Response:\n{response_content}")

    # Example 2
    prompt_2 = "Explain the concept of recursion in programming."
    response_content_2 = call_mistral_and_log(prompt_2)
    if response_content_2:
        print(f"\nMistral Response 2:\n{response_content_2}")
```

**Method 2: Using `requests` Library**

If you are making direct HTTP requests:

```python
import requests
import json
import os
import csv
import datetime
import threading

# --- Configuration ---
API_KEY = os.environ.get("MISTRAL_API_KEY", "YOUR_MISTRAL_API_KEY") # Replace or use env var
MODEL_NAME = "mistral-small-latest"
MISTRAL_API_URL = "https://api.mistral.ai/v1/chat/completions"
LOG_FILE = "mistral_usage_log.csv" # Same log file as Method 1 is fine
log_lock = threading.Lock() # Use lock if making concurrent calls
# --- End Configuration ---

def log_usage_to_csv(model, prompt_tokens, completion_tokens, total_tokens):
    """Appends usage data to the CSV log file. (Same function as above)"""
    timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat()
    fieldnames = ["TimestampUTC", "Model", "PromptTokens", "CompletionTokens", "TotalTokens"]
    data_row = {
        "TimestampUTC": timestamp,
        "Model": model,
        "PromptTokens": prompt_tokens,
        "CompletionTokens": completion_tokens,
        "TotalTokens": total_tokens,
    }
    with log_lock: # Ensure thread-safe file writing
        file_exists = os.path.isfile(LOG_FILE)
        with open(LOG_FILE, 'a', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            if not file_exists or os.path.getsize(LOG_FILE) == 0:
                writer.writeheader() # Write header only if file is new/empty
            writer.writerow(data_row)
    print(f"Logged usage to {LOG_FILE}")


def call_mistral_requests_and_log(user_prompt):
    """Makes an API call using requests and logs usage."""
    headers = {
        'Authorization': f'Bearer {API_KEY}',
        'Content-Type': 'application/json',
        'Accept': 'application/json',
    }
    payload = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": user_prompt}],
        # Add other parameters like temperature, max_tokens if needed
    }

    try:
        response = requests.post(MISTRAL_API_URL, headers=headers, json=payload)
        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

        response_data = response.json()

        # --- Usage Monitoring ---
        if 'usage' in response_data and 'model' in response_data:
            model_used = response_data['model']
            prompt_tokens = response_data['usage']['prompt_tokens']
            completion_tokens = response_data['usage']['completion_tokens']
            total_tokens = response_data['usage']['total_tokens']

            print(f"Model Used: {model_used}")
            print(f"Prompt Tokens: {prompt_tokens}")
            print(f"Completion Tokens: {completion_tokens}")
            print(f"Total Tokens: {total_tokens}")

            # Log this usage to CSV
            log_usage_to_csv(model_used, prompt_tokens, completion_tokens, total_tokens)
            # --- End Usage Monitoring ---

            return response_data['choices'][0]['message']['content']
        else:
            print("Error: 'usage' or 'model' field not found in response.")
            print("Response JSON:", response_data) # Print response for debugging
            return None


    except requests.exceptions.RequestException as e:
        print(f"Error calling Mistral API via requests: {e}")
        if e.response is not None:
             print(f"Response status code: {e.response.status_code}")
             print(f"Response text: {e.response.text}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

# --- Example Usage ---
if __name__ == "__main__":
    prompt = "What is the best way to learn Python?"
    response_content = call_mistral_requests_and_log(prompt)
    if response_content:
        print(f"\nMistral Response:\n{response_content}")
```

## Logging to CSV

The provided Python scripts include a function `log_usage_to_csv`. Key aspects:

1.  **File Handling:** Opens the specified `LOG_FILE` in append mode (`'a'`). This ensures previous logs aren't overwritten.
2.  **CSV Writer:** Uses Python's built-in `csv` module (`DictWriter`) for easy writing.
3.  **Header:** Writes the header row (`TimestampUTC`, `Model`, `PromptTokens`, `CompletionTokens`, `TotalTokens`) only if the file is newly created or empty.
4.  **Timestamp:** Records the time of the API call in UTC for consistency.
5.  **Thread Safety:** Includes a `threading.Lock` (`log_lock`) around file writing. This is important if your application might make multiple Mistral calls concurrently, preventing garbled CSV output.

## Mandatory Log Format

Ensure your submitted `mistral_usage_log.csv` file contains at least these columns, in this order:

1.  `TimestampUTC`: The UTC timestamp when the API call usage was logged (ISO 8601 format recommended, e.g., `2023-10-27T10:30:00.123456+00:00`).
2.  `Model`: The specific Mistral model identifier returned by the API (e.g., `mistral-small-latest`).
3.  `PromptTokens`: The integer number of prompt tokens used.
4.  `CompletionTokens`: The integer number of completion tokens generated.
5.  `TotalTokens`: The integer sum of prompt and completion tokens.

**Example CSV Output (`mistral_usage_log.csv`):**

```csv
TimestampUTC,Model,PromptTokens,CompletionTokens,TotalTokens
2023-10-27T10:30:01.123456+00:00,mistral-small-latest,15,8,23
2023-10-27T10:31:05.654321+00:00,mistral-small-latest,35,150,185
2023-10-27T10:32:10.987654+00:00,mistral-medium-latest,20,45,65
```

## Best Practices

*   **Log After Success:** Only log token usage *after* a successful API call where you receive a valid response containing the `usage` object. Don't log usage for failed API requests.
*   **Integrate Logging:** Call your `log_usage_to_csv` function immediately after processing the successful API response.
*   **Single Log File:** Use one consistent CSV file (`mistral_usage_log.csv`) for all your team's Mistral API calls.
*   **Submission:** Make sure this CSV file is included in your final hackathon submission as requested by the organizers.
*   **Error Handling:** Wrap your API calls in `try...except` blocks to handle potential network issues or API errors gracefully.

By following these steps, you can effectively monitor your Mistral token usage and meet the hackathon requirements. Good luck!
